{
  "id": "fue-03-load-performance-tests",
  "title": "Last-, Performance- und Stresstests",
  "description": "Verstehe die Unterschiede zwischen Last-, Performance- und Stresstests, lerne wichtige Metriken und Tools kennen.",
  "category": "FÜ-03",
  "subcategory": "Durchführen von qualitätssichernden Maßnahmen - Performance-Testing",
  "difficulty": "intermediate",
  "examRelevance": "high",
  "newIn2025": true,
  "removedIn2025": false,
  "important": true,
  "estimatedTime": "50 Minuten",
  "prerequisites": [
    "fue-03-quality"
  ],
  "tags": [
    "lasttest",
    "performancetest",
    "stresstest",
    "testing",
    "qualitätssicherung",
    "metriken",
    "tools"
  ],
  "content": "# Last-, Performance- und Stresstests\n\n## Übersicht\n\n**Performance-Testing** ist ein Oberbegriff für verschiedene Testarten, die das Verhalten eines Systems unter Last untersuchen. Ab 2025 sind Last- und Performancetests explizit im IHK-Prüfungskatalog aufgeführt.\n\n### Warum Performance-Testing?\n\n- ✅ Identifiziere Engpässe vor Produktivbetrieb\n- ✅ Stelle sicher, dass SLAs (Service Level Agreements) eingehalten werden\n- ✅ Optimiere Ressourcennutzung\n- ✅ Verhindere Systemausfälle unter Last\n- ✅ Verbessere Benutzererfahrung\n\n<!-- micro-quiz:fue-03-load-performance-tests-ubersicht-micro-1 -->\n## Arten von Performance-Tests\n\n### 1. Lasttest (Load Test)\n\n**Definition**: Testet das System unter **erwarteter normaler Last**.\n\n**Ziel**: Überprüfen, ob das System die erwartete Anzahl gleichzeitiger Benutzer bewältigen kann.\n\n**Beispiel-Szenario:**\n```\nE-Commerce-Website:\n- Normale Last: 1.000 gleichzeitige Benutzer\n- Testdauer: 1 Stunde\n- Erwartung: Antwortzeit < 2 Sekunden\n```\n\n**Grafische Darstellung:**\n```\nBenutzer\n  ↑\n  │     ┌─────────────────┐\n  │     │  Normale Last   │\n  │     │                 │\n  │─────┘                 └─────\n  │\n  └────────────────────────────→ Zeit\n```\n\n**Wann durchführen:**\n- Vor jedem Release\n- Nach Performance-Optimierungen\n- Bei Infrastruktur-Änderungen\n\n### 2. Performancetest (Performance Test)\n\n**Definition**: Misst **Geschwindigkeit, Reaktionszeit und Stabilität** unter verschiedenen Lastbedingungen.\n\n**Ziel**: Identifiziere Performance-Probleme und Engpässe.\n\n**Gemessene Metriken:**\n- Antwortzeit (Response Time)\n- Durchsatz (Throughput)\n- Ressourcennutzung (CPU, RAM, Disk I/O)\n- Fehlerrate\n\n**Beispiel-Szenario:**\n```\nAPI-Endpunkt:\n- Teste mit 10, 50, 100, 500, 1000 Requests/Sekunde\n- Messe Antwortzeit bei jeder Last\n- Identifiziere, ab wann Antwortzeit steigt\n```\n\n**Grafische Darstellung:**\n```\nAntwortzeit (ms)\n  ↑\n  │                    ╱\n  │                  ╱\n  │                ╱\n  │              ╱\n  │            ╱\n  │──────────╱\n  └────────────────────────────→ Last (Benutzer)\n       Akzeptabel │ Kritisch\n```\n\n### 3. Stresstest (Stress Test)\n\n**Definition**: Testet das System unter **extremer Last**, die über normale Erwartungen hinausgeht.\n\n**Ziel**: Finde die **Belastungsgrenze** des Systems und teste Verhalten bei Überlastung.\n\n**Beispiel-Szenario:**\n```\nWebserver:\n- Normale Last: 1.000 Benutzer\n- Stresstest: Erhöhe auf 5.000, 10.000, 20.000 Benutzer\n- Frage: Wann bricht das System zusammen?\n- Frage: Erholt sich das System nach Lastspitze?\n```\n\n**Grafische Darstellung:**\n```\nBenutzer\n  ↑\n  │           ╱╲\n  │          ╱  ╲\n  │         ╱    ╲\n  │        ╱      ╲\n  │───────╱        ╲───────\n  │\n  └────────────────────────────→ Zeit\n       Ramp-up  │  Ramp-down\n```\n\n**Wann durchführen:**\n- Vor großen Events (Black Friday, Produktlaunch)\n- Nach Architektur-Änderungen\n- Für Disaster-Recovery-Planung\n\n### 4. Weitere Test-Typen\n\n#### Spike Test\n**Definition**: Plötzliche, extreme Lastspitzen.\n\n```\nBenutzer\n  ↑\n  │        ││\n  │        ││\n  │        ││\n  │────────││────────\n  └────────────────────→ Zeit\n```\n\n**Beispiel**: Ticket-Verkauf für Konzert startet\n\n#### Soak Test (Dauertest)\n**Definition**: System über **lange Zeit** unter normaler Last testen.\n\n```\nBenutzer\n  ↑\n  │  ┌────────────────────────┐\n  │  │   Konstante Last       │\n  │──┘                        └──\n  └────────────────────────────→ Zeit\n       24h - 72h\n```\n\n**Ziel**: Memory Leaks, Ressourcen-Erschöpfung finden\n\n#### Scalability Test\n**Definition**: Testet, wie gut das System **skaliert**.\n\n**Fragen:**\n- Verdoppelt sich Durchsatz bei doppelter Hardware?\n- Wie verhält sich das System bei horizontaler Skalierung?\n\n## Wichtige Metriken\n<!-- micro-quiz:fue-03-load-performance-tests-arten-von-performance-tests-micro-1 -->\n\n\n### 1. Antwortzeit (Response Time)\n\n**Definition**: Zeit von Request bis Response.\n\n**Komponenten:**\n```\nGesamte Antwortzeit = Netzwerk + Server-Verarbeitung + Datenbank + Netzwerk\n```\n\n**Typische Werte:**\n- **Exzellent**: < 100 ms\n- **Gut**: 100-300 ms\n- **Akzeptabel**: 300-1000 ms\n- **Langsam**: 1-3 Sekunden\n- **Inakzeptabel**: > 3 Sekunden\n\n### 2. Durchsatz (Throughput)\n\n**Definition**: Anzahl verarbeiteter Requests pro Zeiteinheit.\n\n**Einheiten:**\n- Requests/Sekunde (RPS)\n- Transaktionen/Sekunde (TPS)\n- MB/Sekunde (Datendurchsatz)\n\n**Beispiel:**\n```\nAPI-Endpunkt:\n- 1000 Requests in 10 Sekunden\n- Durchsatz = 100 RPS\n```\n\n### 3. Fehlerrate (Error Rate)\n\n**Definition**: Prozentsatz fehlgeschlagener Requests.\n\n**Berechnung:**\n```\nFehlerrate = (Fehlerhafte Requests / Gesamt Requests) × 100%\n```\n\n**Akzeptable Werte:**\n- **Exzellent**: < 0.1%\n- **Gut**: 0.1-1%\n- **Kritisch**: > 1%\n\n### 4. Ressourcennutzung\n\n**CPU-Auslastung:**\n- **Normal**: < 70%\n- **Hoch**: 70-90%\n- **Kritisch**: > 90%\n\n**RAM-Auslastung:**\n- **Normal**: < 80%\n- **Kritisch**: > 90%\n\n**Disk I/O:**\n- Lese-/Schreibgeschwindigkeit\n- Queue-Länge\n\n**Netzwerk:**\n- Bandbreite-Nutzung\n- Latenz\n- Packet Loss\n\n### 5. Concurrent Users (Gleichzeitige Benutzer)\n\n**Definition**: Anzahl Benutzer, die gleichzeitig aktiv sind.\n\n**Unterschied zu Total Users:**\n```\nTotal Users: 10.000 registrierte Benutzer\nConcurrent Users: 500 gleichzeitig online\n```\n\n### 6. Percentiles (Perzentile)\n\n**Definition**: Statistische Verteilung der Antwortzeiten.\n\n**Wichtige Percentiles:**\n- **P50 (Median)**: 50% der Requests sind schneller\n- **P90**: 90% der Requests sind schneller\n- **P95**: 95% der Requests sind schneller\n- **P99**: 99% der Requests sind schneller\n\n**Beispiel:**\n```\nAntwortzeiten:\nP50 = 200 ms  (Median)\nP90 = 500 ms  (90% unter 500ms)\nP95 = 800 ms\nP99 = 2000 ms (1% sind sehr langsam)\n```\n\n**Warum wichtig?**\n- Durchschnitt kann täuschen (wenige sehr langsame Requests)\n- P95/P99 zeigen Worst-Case-Szenarien\n\n<!-- micro-quiz:fue-03-load-performance-tests-wichtige-metriken-micro-1 -->\n## Performance-Testing-Tools\n\n### 1. Apache JMeter\n\n**Typ**: Open Source, Java-basiert\n\n**Features:**\n- ✅ GUI für Test-Erstellung\n- ✅ HTTP, HTTPS, FTP, JDBC, SOAP, REST\n- ✅ Verteiltes Testen\n- ✅ Umfangreiche Reporting\n\n**Verwendung:**\n```bash\n# JMeter im GUI-Modus starten\njmeter\n\n# JMeter im CLI-Modus (für CI/CD)\njmeter -n -t testplan.jmx -l results.jtl -e -o report/\n```\n\n**Vorteile:**\n- Kostenlos\n- Große Community\n- Viele Plugins\n\n**Nachteile:**\n- Ressourcen-intensiv\n- Steile Lernkurve\n\n### 2. Gatling\n\n**Typ**: Open Source, Scala-basiert\n\n**Features:**\n- ✅ Code-basierte Tests (Scala DSL)\n- ✅ Hohe Performance\n- ✅ Schöne HTML-Reports\n- ✅ CI/CD-Integration\n\n**Beispiel:**\n```scala\nimport io.gatling.core.Predef._\nimport io.gatling.http.Predef._\n\nclass BasicSimulation extends Simulation {\n  val httpProtocol = http\n    .baseUrl(\"https://api.example.com\")\n    \n  val scn = scenario(\"Basic Load Test\")\n    .exec(http(\"Get Users\")\n      .get(\"/users\")\n      .check(status.is(200)))\n    \n  setUp(\n    scn.inject(rampUsers(1000) during (60 seconds))\n  ).protocols(httpProtocol)\n}\n```\n\n### 3. Locust\n\n**Typ**: Open Source, Python-basiert\n\n**Features:**\n- ✅ Python-Code für Tests\n- ✅ Web-UI für Monitoring\n- ✅ Verteiltes Testen\n- ✅ Einfach zu lernen\n\n**Beispiel:**\n```python\nfrom locust import HttpUser, task, between\n\nclass WebsiteUser(HttpUser):\n    wait_time = between(1, 5)\n    \n    @task\n    def load_homepage(self):\n        self.client.get(\"/\")\n    \n    @task(3)  # 3x häufiger als homepage\n    def load_products(self):\n        self.client.get(\"/products\")\n```\n\n### 4. k6\n\n**Typ**: Open Source, Go-basiert\n\n**Features:**\n- ✅ JavaScript für Tests\n- ✅ CLI-fokussiert\n- ✅ Cloud-Integration\n- ✅ Moderne Metriken\n\n**Beispiel:**\n```javascript\nimport http from 'k6/http';\nimport { check, sleep } from 'k6';\n\nexport let options = {\n  stages: [\n    { duration: '2m', target: 100 },  // Ramp-up\n    { duration: '5m', target: 100 },  // Stay\n    { duration: '2m', target: 0 },    // Ramp-down\n  ],\n};\n\nexport default function () {\n  let response = http.get('https://api.example.com/users');\n  check(response, {\n    'status is 200': (r) => r.status === 200,\n    'response time < 500ms': (r) => r.timings.duration < 500,\n  });\n  sleep(1);\n}\n```\n\n### 5. Kommerzielle Tools\n\n#### LoadRunner (Micro Focus)\n- Enterprise-Level\n- Sehr teuer\n- Umfangreiche Features\n\n#### BlazeMeter\n- Cloud-basiert\n- JMeter-kompatibel\n- Skalierbar\n\n#### New Relic / Datadog\n- Application Performance Monitoring (APM)\n- Real User Monitoring (RUM)\n- Produktiv-Monitoring\n\n## Test-Strategie\n<!-- micro-quiz:fue-03-load-performance-tests-performance-testing-tools-micro-1 -->\n\n\n### 1. Testplanung\n\n**Fragen klären:**\n- Was sind die Performance-Ziele? (SLAs)\n- Wie viele Benutzer werden erwartet?\n- Welche Szenarien sind kritisch?\n- Welche Metriken sind wichtig?\n\n**Beispiel-SLA:**\n```\nE-Commerce-Website:\n- 95% der Requests < 2 Sekunden\n- 99% der Requests < 5 Sekunden\n- Fehlerrate < 0.1%\n- Verfügbarkeit > 99.9%\n```\n\n### 2. Test-Szenarien definieren\n\n**Realistische Benutzer-Szenarien:**\n```\nSzenario 1: Produktsuche (40% der Benutzer)\n  1. Homepage aufrufen\n  2. Suchbegriff eingeben\n  3. Suchergebnisse anzeigen\n  4. Produkt auswählen\n\nSzenario 2: Checkout (20% der Benutzer)\n  1. Produkt in Warenkorb\n  2. Zur Kasse\n  3. Adresse eingeben\n  4. Zahlung abschließen\n\nSzenario 3: Browsen (40% der Benutzer)\n  1. Homepage\n  2. Kategorie auswählen\n  3. Produkte durchblättern\n```\n\n### 3. Testumgebung\n\n**Anforderungen:**\n- ✅ Produktionsähnlich (Hardware, Netzwerk)\n- ✅ Isoliert (keine anderen Tests parallel)\n- ✅ Reproduzierbar (gleiche Daten, Konfiguration)\n- ✅ Monitoring aktiviert\n\n### 4. Test-Durchführung\n\n**Phasen:**\n```\n1. Baseline-Test\n   - Einzelner Benutzer\n   - Misst Basis-Performance\n\n2. Ramp-up\n   - Langsam Last erhöhen\n   - Beobachte Verhalten\n\n3. Steady State\n   - Konstante Last\n   - Längere Dauer (30-60 Min)\n\n4. Ramp-down\n   - Last reduzieren\n   - Prüfe Erholung\n\n5. Spike-Test\n   - Plötzliche Lastspitze\n   - Prüfe Stabilität\n```\n\n### 5. Analyse und Reporting\n\n**Zu analysierende Daten:**\n- Antwortzeiten (P50, P95, P99)\n- Durchsatz\n- Fehlerrate\n- Ressourcennutzung\n- Bottlenecks\n\n**Report-Struktur:**\n```\n1. Executive Summary\n   - Ziele erreicht?\n   - Kritische Probleme\n\n2. Test-Konfiguration\n   - Szenarien\n   - Last-Profile\n   - Umgebung\n\n3. Ergebnisse\n   - Metriken\n   - Grafiken\n   - Vergleich mit SLAs\n\n4. Bottlenecks\n   - Identifizierte Probleme\n   - Ursachen\n\n5. Empfehlungen\n   - Optimierungen\n   - Nächste Schritte\n```\n\n<!-- micro-quiz:fue-03-load-performance-tests-test-strategie-micro-1 -->\n## Häufige Bottlenecks\n\n### 1. Datenbank\n\n**Symptome:**\n- Langsame Queries\n- Hohe DB-CPU\n- Connection Pool erschöpft\n\n**Lösungen:**\n- Indizes optimieren\n- Query-Optimierung\n- Caching\n- Read Replicas\n\n### 2. Application Server\n\n**Symptome:**\n- Hohe CPU-Last\n- Langsame Business-Logik\n- Memory Leaks\n\n**Lösungen:**\n- Code-Optimierung\n- Profiling\n- Horizontale Skalierung\n- Caching\n\n### 3. Netzwerk\n\n**Symptome:**\n- Hohe Latenz\n- Bandbreiten-Limit\n- Packet Loss\n\n**Lösungen:**\n- CDN verwenden\n- Kompression\n- HTTP/2 oder HTTP/3\n- Geografische Verteilung\n\n### 4. Frontend\n\n**Symptome:**\n- Langsames Rendering\n- Große Bundle-Größen\n- Viele HTTP-Requests\n\n**Lösungen:**\n- Code-Splitting\n- Lazy Loading\n- Minification\n- Image-Optimierung\n\n## Best Practices\n<!-- micro-quiz:fue-03-load-performance-tests-haufige-bottlenecks-micro-1 -->\n\n\n### 1. Früh testen\n- Performance-Tests in CI/CD integrieren\n- Nicht erst vor Release testen\n\n### 2. Realistische Daten\n- Produktionsähnliche Datenmengen\n- Realistische Benutzer-Szenarien\n\n### 3. Monitoring\n- Detailliertes Monitoring während Tests\n- APM-Tools verwenden\n\n### 4. Iterativ optimieren\n- Bottleneck finden → Optimieren → Erneut testen\n- Dokumentiere Änderungen\n\n### 5. Automatisierung\n- Tests automatisieren\n- Regelmäßig ausführen\n- Trend-Analyse\n\n<!-- micro-quiz:fue-03-load-performance-tests-best-practices-micro-1 -->\n## Zusammenfassung\n\n### Test-Typen\n\n| Test-Typ | Ziel | Last | Dauer |\n|----------|------|------|-------|\n| **Lasttest** | Normale Last bewältigen | Erwartet | Mittel (1-2h) |\n| **Performancetest** | Engpässe finden | Variabel | Kurz-Mittel |\n| **Stresstest** | Belastungsgrenze finden | Extrem | Kurz |\n| **Spike Test** | Lastspitzen bewältigen | Plötzlich hoch | Sehr kurz |\n| **Soak Test** | Stabilität über Zeit | Normal | Lang (24-72h) |\n\n### Wichtige Metriken\n\n- **Antwortzeit**: P50, P95, P99\n- **Durchsatz**: Requests/Sekunde\n- **Fehlerrate**: < 0.1%\n- **Ressourcen**: CPU, RAM, Disk, Netzwerk\n\n### Tools\n\n- **JMeter**: Vielseitig, GUI\n- **Gatling**: Code-basiert, performant\n- **Locust**: Python, einfach\n- **k6**: Modern, CLI\n\n### Für die IHK-Prüfung wichtig:\n\n- Verstehe Unterschiede zwischen Last-, Performance- und Stresstest\n- Kenne wichtige Metriken (Antwortzeit, Durchsatz, Fehlerrate)\n- Verstehe Percentiles (P95, P99)\n- Kenne gängige Tools (JMeter, Gatling, Locust)\n- Erkenne typische Bottlenecks (DB, Server, Netzwerk)\n\n\n\n<!-- micro-quiz:fue-03-load-performance-tests-quiz -->\n<!-- micro-quiz:fue-03-load-performance-tests-zusammenfassung-micro-1 -->\n",
  "codeExamples": [
    {
      "language": "python",
      "title": "Locust: Einfacher Lasttest",
      "code": "from locust import HttpUser, task, between\nimport random\n\nclass ECommerceUser(HttpUser):\n    \"\"\"Simuliert E-Commerce-Benutzer\"\"\"\n    \n    # Wartezeit zwischen Tasks (1-5 Sekunden)\n    wait_time = between(1, 5)\n    \n    def on_start(self):\n        \"\"\"Wird beim Start jedes Benutzers ausgeführt\"\"\"\n        # Login\n        self.client.post(\"/login\", json={\n            \"username\": f\"user{random.randint(1, 1000)}\",\n            \"password\": \"test123\"\n        })\n    \n    @task(3)  # 3x häufiger als andere Tasks\n    def browse_products(self):\n        \"\"\"Produkte durchsuchen\"\"\"\n        # Homepage\n        self.client.get(\"/\")\n        \n        # Kategorie auswählen\n        categories = [\"electronics\", \"books\", \"clothing\"]\n        category = random.choice(categories)\n        self.client.get(f\"/category/{category}\")\n        \n        # Produkt anzeigen\n        product_id = random.randint(1, 100)\n        self.client.get(f\"/product/{product_id}\")\n    \n    @task(2)\n    def search_products(self):\n        \"\"\"Produktsuche\"\"\"\n        search_terms = [\"laptop\", \"phone\", \"book\", \"shirt\"]\n        term = random.choice(search_terms)\n        \n        self.client.get(f\"/search?q={term}\")\n    \n    @task(1)  # Seltener als andere Tasks\n    def add_to_cart(self):\n        \"\"\"Produkt in Warenkorb\"\"\"\n        product_id = random.randint(1, 100)\n        \n        response = self.client.post(\"/cart/add\", json={\n            \"product_id\": product_id,\n            \"quantity\": random.randint(1, 3)\n        })\n        \n        # Prüfe Antwort\n        if response.status_code == 200:\n            print(f\"Produkt {product_id} zum Warenkorb hinzugefügt\")\n    \n    @task(1)\n    def view_cart(self):\n        \"\"\"Warenkorb anzeigen\"\"\"\n        self.client.get(\"/cart\")\n\n# Ausführen:\n# locust -f loadtest.py --host=https://example.com\n# Dann Browser öffnen: http://localhost:8089",
      "explanation": "Vollständiger Locust-Lasttest für E-Commerce-Website. Simuliert realistische Benutzer-Szenarien mit verschiedenen Gewichtungen."
    },
    {
      "language": "javascript",
      "title": "k6: Performance-Test mit Thresholds",
      "code": "import http from 'k6/http';\nimport { check, sleep } from 'k6';\nimport { Rate } from 'k6/metrics';\n\n// Custom Metric für Fehlerrate\nlet errorRate = new Rate('errors');\n\n// Test-Konfiguration\nexport let options = {\n  // Last-Profile\n  stages: [\n    { duration: '1m', target: 50 },   // Ramp-up zu 50 Benutzern\n    { duration: '3m', target: 50 },   // Bleibe bei 50 Benutzern\n    { duration: '1m', target: 100 },  // Erhöhe auf 100 Benutzer\n    { duration: '3m', target: 100 },  // Bleibe bei 100 Benutzern\n    { duration: '1m', target: 0 },    // Ramp-down\n  ],\n  \n  // Thresholds (Test schlägt fehl, wenn nicht erfüllt)\n  thresholds: {\n    'http_req_duration': ['p(95)<500', 'p(99)<1000'],  // 95% < 500ms, 99% < 1s\n    'http_req_failed': ['rate<0.01'],                   // Fehlerrate < 1%\n    'errors': ['rate<0.05'],                            // Custom Error Rate < 5%\n  },\n};\n\nexport default function () {\n  // Test-Szenario: API-Endpunkte testen\n  \n  // 1. Get Users\n  let usersResponse = http.get('https://api.example.com/users');\n  let usersCheck = check(usersResponse, {\n    'users status is 200': (r) => r.status === 200,\n    'users response time < 500ms': (r) => r.timings.duration < 500,\n    'users has data': (r) => r.json().length > 0,\n  });\n  errorRate.add(!usersCheck);\n  \n  sleep(1);\n  \n  // 2. Get specific user\n  let userId = Math.floor(Math.random() * 100) + 1;\n  let userResponse = http.get(`https://api.example.com/users/${userId}`);\n  let userCheck = check(userResponse, {\n    'user status is 200': (r) => r.status === 200,\n    'user has id': (r) => r.json().id !== undefined,\n  });\n  errorRate.add(!userCheck);\n  \n  sleep(1);\n  \n  // 3. Create post\n  let postPayload = JSON.stringify({\n    title: 'Test Post',\n    body: 'This is a test post',\n    userId: userId,\n  });\n  \n  let postResponse = http.post('https://api.example.com/posts', postPayload, {\n    headers: { 'Content-Type': 'application/json' },\n  });\n  \n  let postCheck = check(postResponse, {\n    'post status is 201': (r) => r.status === 201,\n    'post created': (r) => r.json().id !== undefined,\n  });\n  errorRate.add(!postCheck);\n  \n  sleep(2);\n}\n\n// Ausführen:\n// k6 run loadtest.js\n\n// Mit Cloud-Reporting:\n// k6 run --out cloud loadtest.js",
      "explanation": "k6-Performance-Test mit Thresholds und Custom Metrics. Zeigt realistische API-Tests mit Validierung."
    },
    {
      "language": "java",
      "title": "JMeter: Programmatischer Test",
      "code": "import org.apache.jmeter.control.LoopController;\nimport org.apache.jmeter.engine.StandardJMeterEngine;\nimport org.apache.jmeter.protocol.http.sampler.HTTPSampler;\nimport org.apache.jmeter.reporters.ResultCollector;\nimport org.apache.jmeter.reporters.Summariser;\nimport org.apache.jmeter.testelement.TestPlan;\nimport org.apache.jmeter.threads.ThreadGroup;\nimport org.apache.jmeter.util.JMeterUtils;\nimport org.apache.jorphan.collections.HashTree;\n\nimport java.io.File;\n\npublic class JMeterLoadTest {\n    \n    public static void main(String[] args) throws Exception {\n        // JMeter initialisieren\n        String jmeterHome = \"/path/to/jmeter\";\n        JMeterUtils.setJMeterHome(jmeterHome);\n        JMeterUtils.loadJMeterProperties(jmeterHome + \"/bin/jmeter.properties\");\n        JMeterUtils.initLocale();\n        \n        // Test-Plan erstellen\n        TestPlan testPlan = new TestPlan(\"API Load Test\");\n        testPlan.setProperty(TestPlan.FUNCTIONAL_MODE, false);\n        testPlan.setProperty(TestPlan.SERIALIZE_THREADGROUPS, false);\n        \n        // Thread Group (Benutzer-Simulation)\n        ThreadGroup threadGroup = new ThreadGroup();\n        threadGroup.setName(\"API Users\");\n        threadGroup.setNumThreads(100);  // 100 gleichzeitige Benutzer\n        threadGroup.setRampUp(60);       // Ramp-up über 60 Sekunden\n        \n        // Loop Controller\n        LoopController loopController = new LoopController();\n        loopController.setLoops(10);  // Jeder Benutzer führt 10 Iterationen aus\n        loopController.setFirst(true);\n        loopController.initialize();\n        threadGroup.setSamplerController(loopController);\n        \n        // HTTP Sampler (Request)\n        HTTPSampler httpSampler = new HTTPSampler();\n        httpSampler.setDomain(\"api.example.com\");\n        httpSampler.setPort(443);\n        httpSampler.setProtocol(\"https\");\n        httpSampler.setPath(\"/users\");\n        httpSampler.setMethod(\"GET\");\n        httpSampler.setName(\"Get Users Request\");\n        httpSampler.setFollowRedirects(true);\n        \n        // Assertions hinzufügen (optional)\n        // ResponseAssertion assertion = new ResponseAssertion();\n        // assertion.setTestFieldResponseCode();\n        // assertion.addTestString(\"200\");\n        \n        // Summariser für Console-Output\n        Summariser summer = null;\n        String summariserName = JMeterUtils.getPropDefault(\"summariser.name\", \"summary\");\n        if (summariserName.length() > 0) {\n            summer = new Summariser(summariserName);\n        }\n        \n        // Result Collector (Ergebnisse speichern)\n        ResultCollector logger = new ResultCollector(summer);\n        logger.setFilename(\"results.jtl\");\n        \n        // Test-Hierarchie aufbauen\n        HashTree testPlanTree = new HashTree();\n        HashTree threadGroupHashTree = testPlanTree.add(testPlan, threadGroup);\n        threadGroupHashTree.add(httpSampler);\n        testPlanTree.add(testPlanTree.getArray()[0], logger);\n        \n        // JMeter Engine\n        StandardJMeterEngine jmeter = new StandardJMeterEngine();\n        jmeter.configure(testPlanTree);\n        \n        // Test starten\n        System.out.println(\"Starting Load Test...\");\n        jmeter.run();\n        \n        System.out.println(\"Load Test completed. Results saved to results.jtl\");\n    }\n}",
      "explanation": "Programmatische JMeter-Test-Erstellung in Java. Zeigt, wie Tests ohne GUI erstellt und ausgeführt werden können."
    },
    {
      "language": "python",
      "title": "Performance-Metriken analysieren",
      "code": "import statistics\nimport json\nfrom datetime import datetime\n\nclass PerformanceAnalyzer:\n    \"\"\"Analysiert Performance-Test-Ergebnisse\"\"\"\n    \n    def __init__(self, results_file):\n        \"\"\"Lädt Testergebnisse aus JSON-Datei\"\"\"\n        with open(results_file, 'r') as f:\n            self.results = json.load(f)\n    \n    def calculate_percentiles(self, response_times):\n        \"\"\"Berechnet Percentiles\"\"\"\n        sorted_times = sorted(response_times)\n        n = len(sorted_times)\n        \n        return {\n            'p50': sorted_times[int(n * 0.50)],\n            'p75': sorted_times[int(n * 0.75)],\n            'p90': sorted_times[int(n * 0.90)],\n            'p95': sorted_times[int(n * 0.95)],\n            'p99': sorted_times[int(n * 0.99)],\n        }\n    \n    def analyze_response_times(self):\n        \"\"\"Analysiert Antwortzeiten\"\"\"\n        response_times = [r['response_time'] for r in self.results]\n        \n        percentiles = self.calculate_percentiles(response_times)\n        \n        return {\n            'count': len(response_times),\n            'min': min(response_times),\n            'max': max(response_times),\n            'mean': statistics.mean(response_times),\n            'median': statistics.median(response_times),\n            'stdev': statistics.stdev(response_times) if len(response_times) > 1 else 0,\n            'percentiles': percentiles\n        }\n    \n    def calculate_throughput(self):\n        \"\"\"Berechnet Durchsatz (Requests/Sekunde)\"\"\"\n        timestamps = [datetime.fromisoformat(r['timestamp']) for r in self.results]\n        \n        if len(timestamps) < 2:\n            return 0\n        \n        duration = (max(timestamps) - min(timestamps)).total_seconds()\n        return len(self.results) / duration if duration > 0 else 0\n    \n    def calculate_error_rate(self):\n        \"\"\"Berechnet Fehlerrate\"\"\"\n        total = len(self.results)\n        errors = sum(1 for r in self.results if r['status_code'] >= 400)\n        \n        return (errors / total * 100) if total > 0 else 0\n    \n    def check_sla_compliance(self, sla_config):\n        \"\"\"Prüft SLA-Einhaltung\"\"\"\n        response_analysis = self.analyze_response_times()\n        error_rate = self.calculate_error_rate()\n        \n        compliance = {\n            'p95_response_time': {\n                'actual': response_analysis['percentiles']['p95'],\n                'target': sla_config['p95_max_ms'],\n                'passed': response_analysis['percentiles']['p95'] <= sla_config['p95_max_ms']\n            },\n            'p99_response_time': {\n                'actual': response_analysis['percentiles']['p99'],\n                'target': sla_config['p99_max_ms'],\n                'passed': response_analysis['percentiles']['p99'] <= sla_config['p99_max_ms']\n            },\n            'error_rate': {\n                'actual': error_rate,\n                'target': sla_config['max_error_rate_percent'],\n                'passed': error_rate <= sla_config['max_error_rate_percent']\n            }\n        }\n        \n        return compliance\n    \n    def generate_report(self, sla_config=None):\n        \"\"\"Generiert Performance-Report\"\"\"\n        print(\"\\n\" + \"=\"*60)\n        print(\"PERFORMANCE TEST REPORT\")\n        print(\"=\"*60)\n        \n        # Response Times\n        print(\"\\n--- Response Times ---\")\n        response_analysis = self.analyze_response_times()\n        print(f\"Total Requests: {response_analysis['count']}\")\n        print(f\"Min: {response_analysis['min']:.2f} ms\")\n        print(f\"Max: {response_analysis['max']:.2f} ms\")\n        print(f\"Mean: {response_analysis['mean']:.2f} ms\")\n        print(f\"Median (P50): {response_analysis['median']:.2f} ms\")\n        print(f\"Std Dev: {response_analysis['stdev']:.2f} ms\")\n        print(\"\\nPercentiles:\")\n        for p, value in response_analysis['percentiles'].items():\n            print(f\"  {p.upper()}: {value:.2f} ms\")\n        \n        # Throughput\n        print(\"\\n--- Throughput ---\")\n        throughput = self.calculate_throughput()\n        print(f\"Requests/Second: {throughput:.2f}\")\n        \n        # Error Rate\n        print(\"\\n--- Error Rate ---\")\n        error_rate = self.calculate_error_rate()\n        print(f\"Error Rate: {error_rate:.2f}%\")\n        \n        # SLA Compliance\n        if sla_config:\n            print(\"\\n--- SLA Compliance ---\")\n            compliance = self.check_sla_compliance(sla_config)\n            \n            for metric, data in compliance.items():\n                status = \"✓ PASS\" if data['passed'] else \"✗ FAIL\"\n                print(f\"{metric}: {data['actual']:.2f} (target: {data['target']}) {status}\")\n            \n            all_passed = all(data['passed'] for data in compliance.values())\n            print(f\"\\nOverall SLA Compliance: {'✓ PASSED' if all_passed else '✗ FAILED'}\")\n        \n        print(\"\\n\" + \"=\"*60 + \"\\n\")\n\n# Verwendung\nif __name__ == \"__main__\":\n    # SLA-Konfiguration\n    sla = {\n        'p95_max_ms': 500,\n        'p99_max_ms': 1000,\n        'max_error_rate_percent': 1.0\n    }\n    \n    # Analyse durchführen\n    analyzer = PerformanceAnalyzer('test_results.json')\n    analyzer.generate_report(sla_config=sla)",
      "explanation": "Python-Tool zur Analyse von Performance-Test-Ergebnissen. Berechnet Percentiles, Durchsatz, Fehlerrate und prüft SLA-Compliance."
    }
  ],
  "relatedQuizzes": [
    "fue-03-load-performance-tests-quiz"
  ],
  "resources": [
    {
      "title": "Apache JMeter Documentation",
      "url": "https://jmeter.apache.org/usermanual/index.html",
      "type": "documentation"
    },
    {
      "title": "Gatling Documentation",
      "url": "https://gatling.io/docs/",
      "type": "documentation"
    },
    {
      "title": "Locust Documentation",
      "url": "https://docs.locust.io/",
      "type": "documentation"
    },
    {
      "title": "k6 Documentation",
      "url": "https://k6.io/docs/",
      "type": "documentation"
    }
  ],
  "lastUpdated": "2025-01-05T10:00:00Z",
  "version": "1.0",
  "learningObjectives": [
    "Du kannst lasttest erklären",
    "Du kannst performancetest erklären",
    "Du kannst stresstest erklären",
    "Du kannst testing erklären",
    "Du kannst qualitätssicherung erklären"
  ],
  "summary": "Verstehe die Unterschiede zwischen Last-, Performance- und Stresstests, lerne wichtige Metriken und Tools kennen.",
  "contentOutline": [
    {
      "id": "last--performance--und-stresstests",
      "title": "Last-, Performance- und Stresstests",
      "summary": "Kernaspekt: Last-, Performance- und Stresstests"
    },
    {
      "id": "ubersicht",
      "title": "Übersicht",
      "summary": "Kernaspekt: Übersicht"
    },
    {
      "id": "arten-von-performance-tests",
      "title": "Arten von Performance-Tests",
      "summary": "Kernaspekt: Arten von Performance-Tests"
    },
    {
      "id": "wichtige-metriken",
      "title": "Wichtige Metriken",
      "summary": "Kernaspekt: Wichtige Metriken"
    },
    {
      "id": "performance-testing-tools",
      "title": "Performance-Testing-Tools",
      "summary": "Kernaspekt: Performance-Testing-Tools"
    },
    {
      "id": "jmeter-im-gui-modus-starten",
      "title": "JMeter im GUI-Modus starten",
      "summary": "Kernaspekt: JMeter im GUI-Modus starten"
    },
    {
      "id": "jmeter-im-cli-modus-fur-cicd",
      "title": "JMeter im CLI-Modus (für CI/CD)",
      "summary": "Kernaspekt: JMeter im CLI-Modus (für CI/CD)"
    },
    {
      "id": "test-strategie",
      "title": "Test-Strategie",
      "summary": "Kernaspekt: Test-Strategie"
    },
    {
      "id": "haufige-bottlenecks",
      "title": "Häufige Bottlenecks",
      "summary": "Kernaspekt: Häufige Bottlenecks"
    },
    {
      "id": "best-practices",
      "title": "Best Practices",
      "summary": "Kernaspekt: Best Practices"
    },
    {
      "id": "zusammenfassung",
      "title": "Zusammenfassung",
      "summary": "Kernaspekt: Zusammenfassung"
    }
  ],
  "accessibilityNotes": "Klare Überschriften, kurze Sätze, keine reine Farb-Codierung.",
  "editorNote": "Automatisch ergänzt: Review erforderlich",
  "microQuizzes": [
    "fue-03-load-performance-tests-ubersicht-micro-1",
    "fue-03-load-performance-tests-arten-von-performance-tests-micro-1",
    "fue-03-load-performance-tests-wichtige-metriken-micro-1",
    "fue-03-load-performance-tests-performance-testing-tools-micro-1",
    "fue-03-load-performance-tests-test-strategie-micro-1",
    "fue-03-load-performance-tests-haufige-bottlenecks-micro-1",
    "fue-03-load-performance-tests-best-practices-micro-1",
    "fue-03-load-performance-tests-quiz",
    "fue-03-load-performance-tests-zusammenfassung-micro-1"
  ]
}